{
 "metadata": {
  "name": "",
  "signature": "sha256:2a07e38750f524b0f5856504a045f717a6d9ef4ab56e0e6240f6b57288b971af"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "CLEF-IP2010 Vocabulary Builder"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Build stemmed and lemmatized vocabulary (unigrams + bigrams) from clef-ip2010 corpus and store into DB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_corpus(vocabulary_src):\n",
      "    import sqlite3 as sqlitedb\n",
      "\n",
      "    # load patents text from sqlite DB using only vocabulary_src as main field for vocabulary (e.g., abstract, description, claims...)\n",
      "    corpus = []\n",
      "    patents_query = 'select {0} from patents where {1}!=\\'\\' and description!=\\'\\''.format(vocabulary_src,vocabulary_src)\n",
      "    #patents_query = 'select {0} from patents group by lower(description) having description!=\\'\\''.format(vocabulary_src)\n",
      "    con = sqlitedb.connect('/home/wshalaby/work/patents/patents-similarity/data/CLEF/03-patents-with-5-fixes.db')\n",
      "    #con = sqlitedb.connect('/home/wshalaby/work/patents/patents-similarity/data/CLEF/04-patents-full-revised.db')\n",
      "    #con = sqlitedb.connect('/root/Desktop/data-and-indices/patents-data/CLEF-IP/2010/08-clef-patents.db')\n",
      "    with con:\n",
      "        cur = con.execute(patents_query)\n",
      "        count = 0\n",
      "        while True:\n",
      "            patent = cur.fetchone()\n",
      "            if patent==None or patent[0]==None:\n",
      "                break\n",
      "            corpus.append(patent[0])        \n",
      "            count = count + 1\n",
      "        print 'loaded {0} records.'.format(count)\n",
      "    return corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load inquiry stopwords list from DB\n",
      "def load_stopwords():\n",
      "    import sqlite3 as sqlitedb\n",
      "\n",
      "    stop_words = set()\n",
      "    stopwords_query = 'select stopword from inquiry_stopwords'\n",
      "    con = sqlitedb.connect('/home/wshalaby/work/patents/patents-similarity/data/CLEF/03-patents-with-5-fixes.db')\n",
      "    #con = sqlitedb.connect('/home/wshalaby/work/patents/patents-similarity/data/CLEF/04-patents-full-revised.db')\n",
      "    #con = sqlitedb.connect('/root/Desktop/data-and-indices/patents-data/CLEF-IP/2010/08-clef-patents.db')\n",
      "    with con:\n",
      "        cur = con.execute(stopwords_query)\n",
      "        while True:\n",
      "            stopword = cur.fetchone()\n",
      "            if stopword==None or stopword[0]==None:\n",
      "                break\n",
      "            stop_words.add(stopword[0])\n",
      "    \n",
      "    return stop_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf):    \n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "    # tokenize text\n",
      "    vectorizer = CountVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),stop_words=stop_words)\n",
      "    corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "\n",
      "    # apply minimum term frequency threshold\n",
      "    term_freq = corpus_vectors.sum(axis=0) # sum on culomns to obtain term frequencies\n",
      "    terms_to_remove = []\n",
      "    for k,v in vectorizer.vocabulary_.iteritems():\n",
      "        if(term_freq[0,vectorizer.vocabulary_[k]]<min_tf):\n",
      "            terms_to_remove.append(k)\n",
      "\n",
      "    print 'removing ({0}) terms under tf threshold'.format(len(terms_to_remove))\n",
      "    for k in terms_to_remove:\n",
      "        del vectorizer.vocabulary_[k]\n",
      "\n",
      "    return vectorizer.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_vocabulary(vocabulary,tbl_name):\n",
      "    # save vocabulary in DB for future use\n",
      "    import sqlite3 as sqlitedb\n",
      "\n",
      "    l = []\n",
      "    l.extend([i] for i in vocabulary)\n",
      "    con = sqlitedb.connect('/home/wshalaby/work/patents/patents-similarity/data/CLEF/03-patents-with-5-fixes.db')\n",
      "    #con = sqlitedb.connect('/home/wshalaby/work/patents/patents-similarity/data/CLEF/04-patents-full-revised.db')\n",
      "    #con = sqlitedb.connect('/root/Desktop/data-and-indices/patents-data/CLEF-IP/2010/08-clef-patents.db')\n",
      "    with con:\n",
      "        con.execute('drop table if exists {0}'.format(tbl_name))\n",
      "        con.execute('create table {0}(term text)'.format(tbl_name))\n",
      "        con.executemany('insert into {0}(term) values(?)'.format(tbl_name),l)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_df = 2\n",
      "min_tf = 3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams vocabulary\n",
      "def build_lemmatized_unigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_lemmas_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams stopwords vocabulary\n",
      "def build_lemmatized_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_lemmas_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams vocabulary\n",
      "def build_lemmatized_bigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_lemmas_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build lemmatized unigrams stopwords vocabulary\n",
      "def build_lemmatized_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    tokenizer = LemmaTokenizer()\n",
      "    max_ngram_size = 2\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_lemmas_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed unigrams vocabulary\n",
      "def build_stemmed_unigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer()    \n",
      "    stop_words = {}\n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_stems_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed unigrams stopwords vocabulary\n",
      "def build_stemmed_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer()    \n",
      "    max_ngram_size = 1\n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_stems_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed bigrams vocabulary\n",
      "def build_stemmed_bigrams_vocabulary(corpus,vocabulary_src):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer() \n",
      "    stop_words = {}\n",
      "    max_ngram_size = 2    \n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_stems_bigrams_df{1}_tf{2}'.format(vocabulary_src,vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build stemmed bigrams stopwords vocabulary\n",
      "def build_stemmed_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    tokenizer = StemmingTokenizer() \n",
      "    max_ngram_size = 2    \n",
      "    vocabulary = build_vocabulary(corpus,tokenizer,stop_words,max_ngram_size,min_df,min_tf)\n",
      "    # save to DB\n",
      "    tbl_name = 'clef_2010_{0}_stems_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    save_vocabulary(vocabulary,tbl_name)\n",
      "    print 'done '+tbl_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build(vocabulary_src):\n",
      "    # load clef patents from DB using only vocabulary_src as main field for vocabulary (e.g., abstract, description, claims...)\n",
      "    corpus = load_corpus(vocabulary_src)\n",
      "    \n",
      "    # build vocabulary without stopwords removal\n",
      "    build_lemmatized_unigrams_vocabulary(corpus,vocabulary_src)\n",
      "    build_lemmatized_bigrams_vocabulary(corpus,vocabulary_src)\n",
      "    build_stemmed_unigrams_vocabulary(corpus,vocabulary_src)\n",
      "    build_stemmed_bigrams_vocabulary(corpus,vocabulary_src)\n",
      "\n",
      "    # load inquiry stopwords list\n",
      "    stop_words = load_stopwords()\n",
      "\n",
      "    # build vocabulary with stopwords removal\n",
      "    build_lemmatized_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    build_lemmatized_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    build_stemmed_unigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)\n",
      "    build_stemmed_bigrams_stopwords_vocabulary(corpus,stop_words,vocabulary_src)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build clef abstracts vocabulary\n",
      "vocabulary_src = 'abstract'\n",
      "build(vocabulary_src)\n",
      "\n",
      "# build clef claims vocabulary\n",
      "vocabulary_src = 'claims'\n",
      "build(vocabulary_src)\n",
      "\n",
      "# build clef description vocabulary\n",
      "vocabulary_src = 'description'\n",
      "build(vocabulary_src)\n",
      "\n",
      "print 'done!'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
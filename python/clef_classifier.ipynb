{
 "metadata": {
  "name": "",
  "signature": "sha256:2ced5ed21953252f6ac235811f528402788eb6dc169e6368ee778d6aa3794390"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classify CLEF-IP2010 patents"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Classifiy clef-ip2010 patents using unigrams and bigrams features with different classifiers and report classification results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_corpus(vocabulary_src):\n",
      "    import sqlite3 as sqlitedb    \n",
      "    \n",
      "    # load patents text from sqlite DB using only vocabulary_src as main field for vocabulary (e.g., abstract, description, claims...)\n",
      "    corpus = []\n",
      "    labels = []\n",
      "    patents_query = 'select {0},tags from patents group by lower(description) having description!=\\'\\''.format(vocabulary_src)\n",
      "    con = sqlitedb.connect('/home/wshalaby/work/patents/patents-similarity/data/CLEF/03-patents-with-5-fixes.db')\n",
      "    #con = sqlitedb.connect('/home/wshalaby/work/patents/patents-similarity/data/CLEF/04-patents-full-revised.db')\n",
      "    #con = sqlitedb.connect('/root/Desktop/data-and-indices/patents-data/CLEF-IP/2010/08-clef-patents.db')\n",
      "    with con:\n",
      "        cur = con.execute(patents_query)\n",
      "        while True:\n",
      "            patent = cur.fetchone()\n",
      "            if patent==None or patent[0]==None or patent[1]==None:\n",
      "                break\n",
      "            # retrieve patent text\n",
      "            corpus.append(patent[0])\n",
      "            # retrieve patent ipc classification codes (more that one code separated by space)\n",
      "            tags = patent[1].split(' ')\n",
      "            if len(tags[len(tags)-1])==0:\n",
      "                labels.append(tags[0:len(tags)-1])            \n",
      "            else:\n",
      "                labels.append(tags)        \n",
      "    \n",
      "    # map labels into unique class names\n",
      "    labels_dic = {}\n",
      "    labels_arr = []\n",
      "    for i in range(len(labels)):\n",
      "        for j in range(len(labels[i])):\n",
      "            if labels[i][j] not in labels_dic:\n",
      "                labels_dic[labels[i][j]] = len(labels_arr)\n",
      "                labels_arr.append(labels[i][j])\n",
      "                labels[i][j] = len(labels_arr)-1\n",
      "            else:\n",
      "                labels[i][j] = labels_dic[labels[i][j]]\n",
      "    return {'corpus':corpus,'labels':labels,'labels_dic':labels_dic,'labels_arr':labels_arr}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_vocabulary(tbl_name):\n",
      "    import sqlite3 as sqlitedb\n",
      "    \n",
      "    # load vocabulary from sqlite DB\n",
      "    vocabulary = []\n",
      "    stmt = 'select term from {0}'.format(tbl_name)\n",
      "    con = sqlitedb.connect('/home/wshalaby/work/patents/patents-similarity/data/CLEF/03-patents-with-5-fixes.db')\n",
      "    #con = sqlitedb.connect('/home/wshalaby/work/patents/patents-similarity/data/CLEF/04-patents-full-revised.db')\n",
      "    #con = sqlitedb.connect('/root/Desktop/data-and-indices/patents-data/CLEF-IP/2010/08-clef-patents.db')\n",
      "    with con:\n",
      "        cur = con.execute(stmt)\n",
      "        while True:\n",
      "            term = cur.fetchone()\n",
      "            if term==None or term[0]==None:\n",
      "                break\n",
      "            # retrieve patent text\n",
      "            vocabulary.append(term[0])\n",
      "\n",
      "    print 'loaded ({0}) terms'.format(len(vocabulary))\n",
      "    return vocabulary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load all unigrams from tbl_name_full and only bigrams existing in both tbl_name_full&tbl_name_intersect\n",
      "def load_common_vocabulary(tbl_name_full,tbl_name_intersect,stem_or_lemma):\n",
      "    import sqlite3 as sqlitedb\n",
      "    \n",
      "    # load vocabulary from sqlite DB\n",
      "    vocabulary = []\n",
      "    #stmt = 'select term from {0} where instr(term,\\' \\')=0 union select {1} from {2},{3} where {4}=term'.format(tbl_name_full,stem_or_lemma,tbl_name_full,tbl_name_intersect,stem_or_lemma)\n",
      "    stmt = 'select term from {0} where term not like \\'% %\\' union select {1} from {2},{3} where {4}=term union select bigram from {5},{6} where bigram=term'.format(tbl_name_full,stem_or_lemma,tbl_name_full,tbl_name_intersect,stem_or_lemma,tbl_name_full,tbl_name_intersect)\n",
      "    con = sqlitedb.connect('/home/wshalaby/work/patents/patents-similarity/data/CLEF/03-patents-with-5-fixes.db')\n",
      "    #con = sqlitedb.connect('/home/wshalaby/work/patents/patents-similarity/data/CLEF/04-patents-full-revised.db')\n",
      "    #con = sqlitedb.connect('/root/Desktop/data-and-indices/patents-data/CLEF-IP/2010/08-clef-patents.db')\n",
      "    with con:\n",
      "        cur = con.execute(stmt)\n",
      "        while True:\n",
      "            term = cur.fetchone()\n",
      "            if term==None or term[0]==None:\n",
      "                break\n",
      "            # retrieve patent text\n",
      "            vocabulary.append(term[0])\n",
      "\n",
      "    print 'loaded ({0}) terms'.format(len(vocabulary))\n",
      "    return vocabulary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size):\n",
      "    # tokenize text\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "    from sklearn.feature_extraction.text import TfidfTransformer\n",
      "\n",
      "    # generate corpus vectors\n",
      "    vectorizer = CountVectorizer(min_df=min_df,tokenizer=tokenizer,ngram_range=(1,max_ngram_size),vocabulary=vocabulary,stop_words={})\n",
      "    corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "    \n",
      "    # generate tfidf vectors\n",
      "    transformer = TfidfTransformer()\n",
      "    corpus_tfidf_vectors = transformer.fit_transform(corpus_vectors)\n",
      "\n",
      "    return corpus_tfidf_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# given classifier predictions probabilities, return predictions with top n probabilities > 0.5 for each instance or greatest one if all are <=0.5\n",
      "def get_max_n_pred(pred_proba, n_pred):\n",
      "    import heapq\n",
      "    import numpy\n",
      "    max_n_pred = numpy.ndarray(shape=pred_proba.shape)\n",
      "    for i in range(len(pred_proba)):\n",
      "        largest_n_proba = heapq.nlargest(n_pred,pred_proba[i])\n",
      "        max_n_pred[i] = numpy.array(((pred_proba[i]>0.5) & (pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1))\n",
      "        if max_n_pred[i].sum(axis=0)==0: # at least one label should be returned\n",
      "            max_n_pred[i] = numpy.array(((pred_proba[i]>=max(pred_proba[i])) & 1))\n",
      "    return max_n_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reference: http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification\n",
      "def classify(labels,corpus_tfidf_vectors,test_size,max_labels):\n",
      "    from sklearn.preprocessing import MultiLabelBinarizer\n",
      "    from sklearn.multiclass import OneVsRestClassifier\n",
      "    from sklearn.svm import SVC\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "    from sklearn.naive_bayes import MultinomialNB\n",
      "    from sklearn.cross_validation import train_test_split\n",
      "    from sklearn import metrics\n",
      "        \n",
      "    # binarize the labels\n",
      "    mlb = MultiLabelBinarizer()\n",
      "    labels_binarized = mlb.fit_transform(labels)\n",
      "    \n",
      "    # train/test split\n",
      "    #corpus_tfidf_vectors, labels_binarized = shuffle(corpus_tfidf_vectors, labels_binarized)\n",
      "    x_train, x_test, y_train, y_test = train_test_split(corpus_tfidf_vectors, labels_binarized, test_size=test_size, random_state=1)\n",
      "    \n",
      "    # classify\n",
      "    #cls = OneVsRestClassifier(LogisticRegression(class_weight='auto'))\n",
      "    cls = OneVsRestClassifier(LogisticRegression())\n",
      "    #cls = OneVsRestClassifier(MultinomialNB(alpha=0.01))\n",
      "    #cls = OneVsRestClassifier(SVC(kernel='linear',probability=True,max_iter=1000))\n",
      "    cls.fit(x_train, y_train)\n",
      "\n",
      "    # evaluate\n",
      "    pred_proba = cls.predict_proba(x_test)\n",
      "    #print len(pred_proba[0]) # make sure it is 121\n",
      "    pred_labels = mlb.inverse_transform(get_max_n_pred(pred_proba, max_labels))\n",
      "    actual_labels = mlb.inverse_transform(y_test)\n",
      "    return {'precision':metrics.precision_score(actual_labels, pred_labels, average='micro'),\n",
      "            'recall':metrics.recall_score(actual_labels, pred_labels, average='micro'),\n",
      "            'f1':metrics.f1_score(actual_labels, pred_labels, average='micro')}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max_labels = 4 # use only top 4 probabilities labels as maximum labels per patent is 4\n",
      "min_df = 2\n",
      "min_tf = 3\n",
      "test_set_size = 0.33"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_unigrams(corpus,labels,vocabulary_src,with_stopwords_removal):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    max_ngram_size = 1\n",
      "    tokenizer = LemmaTokenizer()\n",
      "    \n",
      "    # load lemmatized unigrams vocabulary\n",
      "    if with_stopwords_removal==False:\n",
      "        vocabulary_tbl_name = 'clef_2010_{0}_lemmas_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    else:\n",
      "        vocabulary_tbl_name = 'clef_2010_{0}_lemmas_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_tfidf_vectors = vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(labels,corpus_tfidf_vectors,test_set_size,max_labels)\n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_bigrams(corpus,labels,vocabulary_src,with_stopwords_removal):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    tokenizer = LemmaTokenizer()\n",
      "    \n",
      "    # load lemmatized bigrams vocabulary\n",
      "    if with_stopwords_removal==False:\n",
      "        vocabulary_tbl_name = 'clef_2010_{0}_lemmas_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    else:\n",
      "        vocabulary_tbl_name = 'clef_2010_{0}_lemmas_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_tfidf_vectors = vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(labels,corpus_tfidf_vectors,test_set_size,max_labels)\n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_unigrams(corpus,labels,vocabulary_src,with_stopwords_removal):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    max_ngram_size = 1\n",
      "    tokenizer = StemmingTokenizer()\n",
      "    \n",
      "    # load lemmatized unigrams vocabulary\n",
      "    if with_stopwords_removal==False:\n",
      "        vocabulary_tbl_name = 'clef_2010_{0}_stems_unigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    else:\n",
      "        vocabulary_tbl_name = 'clef_2010_{0}_stems_unigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_tfidf_vectors = vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(labels,corpus_tfidf_vectors,test_set_size,max_labels)\n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_bigrams(corpus,labels,vocabulary_src,with_stopwords_removal):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    tokenizer = StemmingTokenizer()\n",
      "    \n",
      "    # load lemmatized bigrams vocabulary\n",
      "    if with_stopwords_removal==False:\n",
      "        vocabulary_tbl_name = 'clef_2010_{0}_stems_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    else:\n",
      "        vocabulary_tbl_name = 'clef_2010_{0}_stems_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    vocabulary = load_vocabulary(vocabulary_tbl_name)\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_tfidf_vectors = vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(labels,corpus_tfidf_vectors,test_set_size,max_labels)\n",
      "    print vocabulary_tbl_name,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_wiki_bigrams(corpus,labels,vocabulary_src,with_stopwords_removal):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    tokenizer = LemmaTokenizer()\n",
      "    \n",
      "    # load lemmatized bigrams vocabulary common with wikpedia bigrams\n",
      "    if with_stopwords_removal==False:\n",
      "        vocabulary_tbl_full = 'clef_2010_{0}_lemmas_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    else:\n",
      "        vocabulary_tbl_full = 'clef_2010_{0}_lemmas_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    vocabulary_tbl_intersect = 'wiki_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_full,vocabulary_tbl_intersect,'lemma')\n",
      "    print 'done loading vocabulary'\n",
      "    # generate tfidf vectors\n",
      "    corpus_tfidf_vectors = vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size)\n",
      "    print 'done vectorizing'\n",
      "    # classify & evaluate    \n",
      "    results = classify(labels,corpus_tfidf_vectors,test_set_size,max_labels)\n",
      "    print vocabulary_tbl_full,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_wiktionary_bigrams(corpus,labels,vocabulary_src,with_stopwords_removal):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    tokenizer = LemmaTokenizer()\n",
      "    \n",
      "    # load lemmatized bigrams vocabulary common with wikpedia bigrams\n",
      "    if with_stopwords_removal==False:\n",
      "        vocabulary_tbl_full = 'clef_2010_{0}_lemmas_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    else:\n",
      "        vocabulary_tbl_full = 'clef_2010_{0}_lemmas_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    vocabulary_tbl_intersect = 'wiktionary_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_full,vocabulary_tbl_intersect,'lemma')\n",
      "    print 'done loading vocabulary'\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_tfidf_vectors = vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size)\n",
      "    print 'done vectorizing'\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(labels,corpus_tfidf_vectors,test_set_size,max_labels)\n",
      "    print vocabulary_tbl_full,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_lemmatized_google_bigrams(corpus,labels,vocabulary_src,with_stopwords_removal):\n",
      "    from lemmatizing_tokenizer import LemmaTokenizer\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    tokenizer = LemmaTokenizer()\n",
      "    \n",
      "    # load lemmatized bigrams vocabulary common with wikpedia bigrams\n",
      "    if with_stopwords_removal==False:\n",
      "        vocabulary_tbl_full = 'clef_2010_{0}_lemmas_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    else:\n",
      "        vocabulary_tbl_full = 'clef_2010_{0}_lemmas_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    vocabulary_tbl_intersect = 'google_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_full,vocabulary_tbl_intersect,'lemma')\n",
      "    print 'done loading vocabulary'\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_tfidf_vectors = vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size)\n",
      "    print 'done vectorizing'\n",
      "    # classify & evaluate    \n",
      "    results = classify(labels,corpus_tfidf_vectors,test_set_size,max_labels)\n",
      "    print vocabulary_tbl_full,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_wiki_bigrams(corpus,labels,vocabulary_src,with_stopwords_removal):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    tokenizer = StemmingTokenizer()\n",
      "    \n",
      "    # load lemmatized bigrams vocabulary common with wikpedia bigrams\n",
      "    if with_stopwords_removal==False:\n",
      "        vocabulary_tbl_full = 'clef_2010_{0}_stems_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    else:\n",
      "        vocabulary_tbl_full = 'clef_2010_{0}_stems_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    vocabulary_tbl_intersect = 'wiki_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_full,vocabulary_tbl_intersect,'stem')\n",
      "    print 'done loading vocabulary'\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_tfidf_vectors = vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size)\n",
      "    print 'done vectorizing'\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(labels,corpus_tfidf_vectors,test_set_size,max_labels)\n",
      "    print vocabulary_tbl_full,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_wiktionary_bigrams(corpus,labels,vocabulary_src,with_stopwords_removal):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    tokenizer = StemmingTokenizer()\n",
      "    \n",
      "    # load lemmatized bigrams vocabulary common with wikpedia bigrams\n",
      "    if with_stopwords_removal==False:\n",
      "        vocabulary_tbl_full = 'clef_2010_{0}_stems_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    else:\n",
      "        vocabulary_tbl_full = 'clef_2010_{0}_stems_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    vocabulary_tbl_intersect = 'wiktionary_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_full,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_tfidf_vectors = vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(labels,corpus_tfidf_vectors,test_set_size,max_labels)\n",
      "    print vocabulary_tbl_full,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_stemmed_google_bigrams(corpus,labels,vocabulary_src,with_stopwords_removal):\n",
      "    from stemming_tokenizer import StemmingTokenizer\n",
      "    \n",
      "    max_ngram_size = 2\n",
      "    tokenizer = StemmingTokenizer()\n",
      "    \n",
      "    # load lemmatized bigrams vocabulary common with wikpedia bigrams\n",
      "    if with_stopwords_removal==False:\n",
      "        vocabulary_tbl_full = 'clef_2010_{0}_stems_bigrams_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    else:\n",
      "        vocabulary_tbl_full = 'clef_2010_{0}_stems_bigrams_stopwords_df{1}_tf{2}'.format(vocabulary_src,min_df,min_tf)\n",
      "    vocabulary_tbl_intersect = 'google_bigrams'\n",
      "    vocabulary = load_common_vocabulary(vocabulary_tbl_full,vocabulary_tbl_intersect,'stem')\n",
      "\n",
      "    # generate tfidf vectors\n",
      "    corpus_tfidf_vectors = vectorize_corpus(corpus,tokenizer,vocabulary,max_ngram_size)\n",
      "    \n",
      "    # classify & evaluate    \n",
      "    results = classify(labels,corpus_tfidf_vectors,test_set_size,max_labels)\n",
      "    print vocabulary_tbl_full,'^',vocabulary_tbl_intersect,' --> ','precision ',results['precision'],'recall ',results['recall'],'f1 ',results['f1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test(vocabulary_src):\n",
      "    # load clef patents from DB using only vocabulary_src as main field for vocabulary (e.g., abstract, description, claims...)\n",
      "    corpus_data = load_corpus(vocabulary_src)\n",
      "    corpus = corpus_data['corpus']\n",
      "    labels = corpus_data['labels']\n",
      "    labels_dic = corpus_data['labels_dic']\n",
      "    labels_arr = corpus_data['labels_arr']\n",
      "    print 'done loading {0} records and {1} labels.'.format(len(corpus),len(labels_dic))\n",
      "\n",
      "    # test without stopword removal\n",
      "    test_lemmatized_unigrams(corpus,labels,vocabulary_src,False)\n",
      "    #test_lemmatized_bigrams(corpus,labels,vocabulary_src,False)\n",
      "    test_lemmatized_wiki_bigrams(corpus,labels,vocabulary_src,False)\n",
      "    test_lemmatized_wiktionary_bigrams(corpus,labels,vocabulary_src,False)\n",
      "    test_lemmatized_google_bigrams(corpus,labels,vocabulary_src,False)\n",
      "    test_stemmed_unigrams(corpus,labels,vocabulary_src,False)\n",
      "    #test_stemmed_bigrams(corpus,labels,vocabulary_src,False)\n",
      "    test_stemmed_wiki_bigrams(corpus,labels,vocabulary_src,False)\n",
      "    test_stemmed_wiktionary_bigrams(corpus,labels,vocabulary_src,False)\n",
      "    test_stemmed_google_bigrams(corpus,labels,vocabulary_src,False)\n",
      "\n",
      "    # test with stopword removal\n",
      "    test_lemmatized_unigrams(corpus,labels,vocabulary_src,True)\n",
      "    #test_lemmatized_bigrams(corpus,labels,vocabulary_src,True)\n",
      "    test_lemmatized_wiki_bigrams(corpus,labels,vocabulary_src,True)\n",
      "    test_lemmatized_wiktionary_bigrams(corpus,labels,vocabulary_src,True)\n",
      "    test_lemmatized_google_bigrams(corpus,labels,vocabulary_src,True)\n",
      "    test_stemmed_unigrams(corpus,labels,vocabulary_src,True)\n",
      "    #test_stemmed_bigrams(corpus,labels,vocabulary_src,True)\n",
      "    test_stemmed_wiki_bigrams(corpus,labels,vocabulary_src,True)\n",
      "    test_stemmed_wiktionary_bigrams(corpus,labels,vocabulary_src,True)\n",
      "    test_stemmed_google_bigrams(corpus,labels,vocabulary_src,True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test using abstracts vocabulary\n",
      "vocabulary_src = 'abstract'\n",
      "test(vocabulary_src)\n",
      "\n",
      "# test using clef claims vocabulary\n",
      "vocabulary_src = 'claims'\n",
      "test(vocabulary_src)\n",
      "\n",
      "# test using clef description vocabulary\n",
      "vocabulary_src = 'description'\n",
      "test(vocabulary_src)\n",
      "\n",
      "print 'done!'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
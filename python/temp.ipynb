{
 "metadata": {
  "name": "",
  "signature": "sha256:3e124a542f2d6fcef69ad0b3844758ddc134530ccca7c4ad6187bcc5717d0346"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare stemmer\n",
      "from nltk import word_tokenize          \n",
      "from nltk.stem import WordNetLemmatizer \n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import PorterStemmer\n",
      "class StermmingTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stemmer = PorterStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.stemmer.stem(tokens.lower()) for tokens in word_tokenize(doc)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize text\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "vectorizer = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',tokenizer=StermmingTokenizer(),ngram_range=(1,1),stop_words={},min_df=1)#(min_df=0.000001,max_df=0.95)\n",
      "corpus = ['hi cairo here is cairo','here usa s 5.here in usa','hi in']\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "\n",
      "print corpus_vectors.shape\n",
      "print vectorizer.vocabulary_\n",
      "print vectorizer.stop_words_\n",
      "print corpus_vectors.todense()\n",
      "corpus_vectors = corpus_vectors.tolil()\n",
      "term_freq = corpus_vectors.sum(axis=0)\n",
      "#print corpus_vectors.sum(axis=1)\n",
      "keys_to_remove = []\n",
      "for k,v in vectorizer.vocabulary_.iteritems():\n",
      "    print k,v,term_freq[0,vectorizer.vocabulary_[k]]\n",
      "    if(term_freq[0,vectorizer.vocabulary_[k]]<0):\n",
      "        corpus_vectors[:,v] = 0\n",
      "        keys_to_remove.append(k)\n",
      "  \n",
      "print corpus_vectors.todense()\n",
      "print keys_to_remove\n",
      "for k in keys_to_remove:\n",
      "    del vectorizer.vocabulary_[k]\n",
      "    \n",
      "print vectorizer.vocabulary_\n",
      "#print sum(corpus_vectors.getcol(vectorizer.vocabulary_[i])).todense()[0,0]>0\n",
      "#print corpus_vectors[:,vectorizer.vocabulary_['cairo']]\n",
      "#print corpus_vectors\n",
      "#print corpus_vectors.shape\n",
      "\n",
      "#binarizer = Binarizer()\n",
      "#corpus_binary_vectors = binarizer.transform(corpus_vectors)\n",
      "#print corpus_binary_vectors\n",
      "#doc_freq = corpus_binary_vectors.sum(axis=0)\n",
      "#keys_to_remove = []\n",
      "#for k,v in vectorizer.vocabulary_.iteritems():\n",
      "#    print k,v,doc_freq[0,vectorizer.vocabulary_[k]]\n",
      "#    if(doc_freq[0,vectorizer.vocabulary_[k]]<0):\n",
      "#        corpus_vectors[:,v] = 0\n",
      "#        keys_to_remove.append(k)\n",
      "  \n",
      "corpus_vectors = corpus_vectors.tocsr()\n",
      "#print corpus_vectors.todense()\n",
      "#print keys_to_remove\n",
      "#for k in keys_to_remove:\n",
      "#    del vectorizer.vocabulary_[k]\n",
      "    \n",
      "print vectorizer.vocabulary_\n",
      "print len(vectorizer.vocabulary_)\n",
      "# tokenize text\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "transformer = TfidfTransformer()\n",
      "titles_tfidf_vectors = transformer.fit_transform(corpus_vectors)\n",
      "\n",
      "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
      "#vectorizer = TfidfVectorizer(lowercase=False)#(min_df=0.000001,max_df=0.95)\n",
      "#corpus = ['hi this is cairo','hi here in usa']\n",
      "#titles_tfidf_vectors = vectorizer.fit_transform(corpus_vectors)\n",
      "\n",
      "#print vectorizer\n",
      "print titles_tfidf_vectors.shape\n",
      "print titles_tfidf_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print pred_labels\n",
      "#print probs\n",
      "#metrics.f1_score(y_test, pred)\n",
      "#correct = 0.0\n",
      "#incorrect = 0.0\n",
      "#for i in range(len(y_test)):\n",
      "#    is_correct = False\n",
      "#    for j in range(len(y_test[i])):\n",
      "#        if pred[i]==y_test[i][j]:\n",
      "#            correct = correct + 1\n",
      "#            is_correct = True\n",
      "#            break        \n",
      "#    if is_correct==False:\n",
      "#        incorrect = incorrect + 1\n",
      "        \n",
      "#print 'total=', len(labels), 'test=', len(y_test), 'correct=', correct, 'incorrect=', incorrect, 'accuracy=', correct/len(y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import PorterStemmer\n",
      "from nltk import RegexpTokenizer\n",
      "\n",
      "class StermmingTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stemmer = PorterStemmer()\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b[a-z\\\\-]+\\\\b')\n",
      "    def __call__(self, doc):\n",
      "        return [self.stemmer.stem(tokens.lower()) for tokens in self.tokenizer.tokenize(doc)]\n",
      "    \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "vectorizer = CountVectorizer(tokenizer=StermmingTokenizer(),ngram_range=(1,1),stop_words={},min_df=2)#(min_df=0.000001,max_df=0.95)\n",
      "corpus = ['hi cairo here is CAIRON','here USA s 09.here in u-sa','hi in']\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "\n",
      "# apply minimum term frequency threshold\n",
      "corpus_vectors = corpus_vectors.tolil()\n",
      "term_freq = corpus_vectors.sum(axis=0) # sum on culomns to obtain term frequencies\n",
      "terms_to_remove = []\n",
      "for k,v in vectorizer.vocabulary_.iteritems():\n",
      "    if(term_freq[0,vectorizer.vocabulary_[k]]<3):\n",
      "        corpus_vectors[:,v] = 0\n",
      "        terms_to_remove.append(k)\n",
      "\n",
      "for k in terms_to_remove:\n",
      "    del vectorizer.vocabulary_[k]\n",
      "\n",
      "print corpus_vectors.shape\n",
      "print vectorizer.vocabulary_\n",
      "print vectorizer.stop_words_\n",
      "#tokenizer=StermmingTokenizer(),\n",
      "#token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(3, 3)\n",
        "{u'here': 0}\n",
        "set([u'usa', u'cairo', u'is', u'cairon', u's', u'u-sa'])\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare stemmer\n",
      "from nltk import RegexpTokenizer\n",
      "from nltk.stem import WordNetLemmatizer \n",
      "from sklearn.feature_selection import chi2\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b[a-z]+\\\\-*[a-z]+|\\\\b(?u)\\\\b[a-z]\\\\b')\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc)]\n",
      "    \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "import heapq\n",
      "import numpy\n",
      "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),ngram_range=(1,2),stop_words={'k','x'},min_df=2,vocabulary={u'cairo', u'in', u'here', u'here in', u'class', u'cairo class','hereis','cx'})#(min_df=0.000001,max_df=0.95)\n",
      "#corpus = ['hi k cairo class classifiable classified doing here is CAIRON','here USA s 09-hers in -x u-sa','hi in']\n",
      "corpus = ['hi cairo class here is CAIRON. animals farms','here USA here in usa','here in cairo classes']\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "y = numpy.array([[0,1,0],[1,1,0],[0,1,1]])\n",
      "y = [0,1,0]\n",
      "chi,pvals = chi2(corpus_vectors,y)\n",
      "print chi\n",
      "print pvals\n",
      "print 'nans',sum(numpy.isnan(chi))\n",
      "largest = heapq.nlargest(3,chi)\n",
      "print largest\n",
      "print min(largest)\n",
      "print chi>=min(largest)\n",
      "chi_ = numpy.array((chi>=min(largest)) & 1)\n",
      "print chi_,sum(chi_)\n",
      "print corpus_vectors.shape\n",
      "print vectorizer.vocabulary_\n",
      "print corpus_vectors\n",
      "ch2 = SelectKBest(chi2, k=3)\n",
      "ch2.fit(corpus_vectors, y)\n",
      "#print 'hoy',ch2.inverse_transform(chi22)\n",
      "print 'he',ch2.get_support() & 1\n",
      "#print 'hi',chi22\n",
      "#print vectorizer.stop_words_\n",
      "#tokenizer=StermmingTokenizer(),\n",
      "#token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.25  1.    1.    0.5    nan   nan  0.25  1.  ]\n",
        "[ 0.61707508  0.31731051  0.31731051  0.47950012         nan         nan\n",
        "  0.61707508  0.31731051]\n",
        "nans 2\n",
        "[1.0, 1.0, 1.0]\n",
        "1.0\n",
        "[False  True  True False False False False  True]\n",
        "[0 1 1 0 0 0 0 1] 3\n",
        "(3, 8)\n",
        "{u'here in': 0, u'cairo class': 1, u'cairo': 2, u'here': 3, 'hereis': 4, 'cx': 5, u'in': 6, u'class': 7}\n",
        "  (0, 1)\t1\n",
        "  (0, 2)\t1\n",
        "  (0, 3)\t1\n",
        "  (0, 7)\t1\n",
        "  (1, 0)\t1\n",
        "  (1, 3)\t2\n",
        "  (1, 6)\t1\n",
        "  (2, 0)\t1\n",
        "  (2, 1)\t1\n",
        "  (2, 2)\t1\n",
        "  (2, 3)\t1\n",
        "  (2, 6)\t1\n",
        "  (2, 7)\t1\n",
        "hehe [ 0.25  1.    1.    0.5    nan   nan  0.25  1.  ]\n",
        "he [0 1 1 0 0 0 0 1]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# alphanumeric tokenizer\n",
      "from nltk import RegexpTokenizer\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "class RawLemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc)]\n",
      "    \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "import heapq\n",
      "import numpy\n",
      "vectorizer = CountVectorizer(tokenizer=RawLemmaTokenizer(),ngram_range=(1,2),stop_words={'k','x'},min_df=1)#(min_df=0.000001,max_df=0.95)\n",
      "#corpus = ['hi k cairo class classifiable classified doing here is CAIRON','here USA s 09-hers in -x u-sa','hi in']\n",
      "corpus = ['hi cairo class here is CAIRON. animals farms','here USA here in usa','here in cairo classes']\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "print corpus_vectors.shape\n",
      "print vectorizer.vocabulary_\n",
      "print corpus_vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(3, 22)\n",
        "{u'in usa': 17, u'class here': 7, u'cairo': 2, u'is': 18, u'in': 15, u'here is': 11, u'here': 9, u'here in': 10, u'hi': 13, u'animal farm': 1, u'class': 6, u'cairona animal': 5, u'usa': 20, u'cairo class': 3, u'in cairo': 16, u'is cairona': 19, u'farm': 8, u'here usa': 12, u'animal': 0, u'usa here': 21, u'cairona': 4, u'hi cairo': 14}\n",
        "  (0, 13)\t1\n",
        "  (0, 2)\t1\n",
        "  (0, 6)\t1\n",
        "  (0, 9)\t1\n",
        "  (0, 18)\t1\n",
        "  (0, 4)\t1\n",
        "  (0, 0)\t1\n",
        "  (0, 8)\t1\n",
        "  (0, 14)\t1\n",
        "  (0, 3)\t1\n",
        "  (0, 7)\t1\n",
        "  (0, 11)\t1\n",
        "  (0, 19)\t1\n",
        "  (0, 5)\t1\n",
        "  (0, 1)\t1\n",
        "  (1, 9)\t2\n",
        "  (1, 20)\t2\n",
        "  (1, 15)\t1\n",
        "  (1, 12)\t1\n",
        "  (1, 21)\t1\n",
        "  (1, 10)\t1\n",
        "  (1, 17)\t1\n",
        "  (2, 2)\t1\n",
        "  (2, 6)\t1\n",
        "  (2, 9)\t1\n",
        "  (2, 3)\t1\n",
        "  (2, 15)\t1\n",
        "  (2, 10)\t1\n",
        "  (2, 16)\t1\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = ['elephant','lion','cat','dog','whale','ant','bee','peagon','eagle','parrot','human']\n",
      "labels = [['mammals','animals'],['mammals','animals'],['mammals','animals'],['mammals','animals'],['mammals','animals'],['nonmammals','animals'],['nonmammals','animals'],['nonmammals','animals'],['nonmammals','animals'],['nonmammals','animals'],['mammals']]\n",
      "labels_dic = {}\n",
      "labels_arr = []\n",
      "for i in range(len(labels)):\n",
      "    for j in range(len(labels[i])):\n",
      "        #x = labels[i][j]\n",
      "        if labels[i][j] not in labels_dic:\n",
      "            labels_dic[labels[i][j]] = len(labels_arr)\n",
      "            labels_arr.append(labels[i][j])\n",
      "            labels[i][j] = len(labels_arr)-1\n",
      "        else:\n",
      "            labels[i][j] = labels_dic[labels[i][j]]\n",
      "        #print x,labels[i][j]\n",
      "\n",
      "print len(labels_arr)\n",
      "print len(corpus)\n",
      "#print labels\n",
      "print labels_arr\n",
      "print labels_dic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare lemmatizer\n",
      "from nltk import RegexpTokenizer\n",
      "from nltk.stem import WordNetLemmatizer \n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b[a-z]+\\\\-*[a-z]+|\\\\b(?u)\\\\b[a-z]\\\\b')\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize text\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "\n",
      "min_term_freq = 1\n",
      "                                           \n",
      "vectorizer = CountVectorizer(min_df=1,tokenizer=LemmaTokenizer(),ngram_range=(1,1),stop_words={})\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "print len(vectorizer.vocabulary_)\n",
      "#print vectorizer.vocabulary_\n",
      "#print vectorizer.stop_words_\n",
      "#print corpus_vectors.todense()\n",
      "corpus_vectors = corpus_vectors.tolil()\n",
      "term_freq = corpus_vectors.sum(axis=0) # sum on culomns to obtain term frequencies\n",
      "terms_to_remove = []\n",
      "for k,v in vectorizer.vocabulary_.iteritems():\n",
      "    #print k,v,term_freq[0,vectorizer.vocabulary_[k]]\n",
      "    if(term_freq[0,vectorizer.vocabulary_[k]]<min_term_freq):\n",
      "        corpus_vectors[:,v] = 0\n",
      "        terms_to_remove.append(k)\n",
      "  \n",
      "#print corpus_vectors.todense()\n",
      "print len(terms_to_remove)\n",
      "for k in terms_to_remove:\n",
      "    del vectorizer.vocabulary_[k]\n",
      "    \n",
      "print vectorizer.vocabulary_\n",
      "#print sum(corpus_vectors.getcol(vectorizer.vocabulary_[i])).todense()[0,0]>0\n",
      "#print corpus_vectors[:,vectorizer.vocabulary_['cairo']]\n",
      "#print corpus_vectors\n",
      "print len(vectorizer.vocabulary_)\n",
      "\n",
      "corpus_vectors = corpus_vectors.tocsr()\n",
      "#print corpus_vectors.todense()\n",
      "#corpus_vectors?\n",
      "    \n",
      "print len(vectorizer.vocabulary_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize text\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "\n",
      "transformer = TfidfTransformer()#(min_df=0.000001,max_df=0.95)\n",
      "#corpus = ['hi this is cairo','hi here in usa']\n",
      "corpus_tfidf_vectors = transformer.fit_transform(corpus_vectors)\n",
      "\n",
      "#print vectorizer\n",
      "print corpus_tfidf_vectors.shape\n",
      "print corpus_tfidf_vectors\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# binarize the labels\n",
      "from sklearn.preprocessing import MultiLabelBinarizer\n",
      "\n",
      "\n",
      "mlb = MultiLabelBinarizer()\n",
      "labels_binarized = mlb.fit_transform(labels)\n",
      "labels_binarized.shape\n",
      "\n",
      "print labels_binarized\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# given classifier predictions probabilities, return predictions with top n probabilities for each instance\n",
      "import heapq\n",
      "import numpy\n",
      "\n",
      "def get_max_n_pred(pred_proba, n_pred):\n",
      "    max_n_pred = numpy.ndarray(shape=pred_proba.shape)\n",
      "    for i in range(len(pred_proba)):\n",
      "        largest_n_proba = heapq.nlargest(n_pred,pred_proba[i])\n",
      "        print \"1\",(pred_proba[i]>0.5)\n",
      "        print \"2\",(pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1])\n",
      "        print \"3\",numpy.array(((pred_proba[i]>0.5) & (pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1))\n",
      "        max_n_pred[i] = numpy.array(((pred_proba[i]>0.5) & (pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1))\n",
      "        if max_n_pred[i].sum(axis=1)==0:\n",
      "            print \"zeros\"\n",
      "            max_n_pred[i] = numpy.array(((pred_proba[i]>=max(pred_proba[i])) & 1))\n",
      "    \n",
      "    return max_n_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# classify\n",
      "from sklearn.multiclass import OneVsRestClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import metrics\n",
      "x_train, x_test, y_train, y_test = train_test_split(corpus_tfidf_vectors, labels_binarized, test_size=0.1, random_state=87)\n",
      "cls = OneVsRestClassifier(LogisticRegression())\n",
      "#cls = OneVsRestClassifier(MultinomialNB(alpha=0.01))\n",
      "#cls = OneVsRestClassifier(SVC(kernel='linear',probability=True))\n",
      "cls.fit(x_train, y_train)\n",
      "\n",
      "# evaluate\n",
      "pred = cls.predict(x_test)\n",
      "pred_proba = cls.predict_proba(x_test)\n",
      "print x_test\n",
      "print \"pred\", pred\n",
      "print \"pred_proba\", pred_proba\n",
      "#for i in range(len(pred_proba)):\n",
      "#    largest_n_proba = heapq.nlargest(4,pred_proba[i])\n",
      "#    pred_proba[i] = numpy.array((pred_proba[i]>=largest_n_proba[len(largest_n_proba)-1]) & 1)\n",
      "    \n",
      "pred_labels = mlb.inverse_transform(get_max_n_pred(pred_proba,1))\n",
      "print \"pred_labels\",pred_labels\n",
      "pred_labels = mlb.inverse_transform(pred)\n",
      "print \"pred_labels again\", pred_labels\n",
      "actual_labels = mlb.inverse_transform(y_test)\n",
      "print \"actual_labels\", actual_labels\n",
      "# http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification\n",
      "print metrics.precision_score(actual_labels, pred_labels, average='weighted')\n",
      "print metrics.recall_score(actual_labels, pred_labels, average='weighted')\n",
      "print metrics.f1_score(actual_labels, pred_labels, average='weighted')\n",
      "#pred_probs = cls.predict_proba(x_test)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "foo = np.asanyarray(range(5))\n",
      "foo1 = np.asanyarray(range(4))\n",
      "print foo\n",
      "print ((foo>2) & (foo1>31))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import PorterStemmer\n",
      "from nltk import RegexpTokenizer\n",
      "import numpy\n",
      "from scipy import sparse\n",
      "\n",
      "class StermmingTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.stemmer = PorterStemmer()\n",
      "        self.tokenizer = RegexpTokenizer(u'(?u)\\\\b[a-z\\\\-]+\\\\b')\n",
      "    def __call__(self, doc):\n",
      "        return [self.stemmer.stem(tokens.lower()) for tokens in self.tokenizer.tokenize(doc)]\n",
      "    \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import Binarizer\n",
      "min_tf = 3\n",
      "vectorizer = CountVectorizer(tokenizer=StermmingTokenizer(),ngram_range=(1,1),stop_words={},min_df=2)#(min_df=0.000001,max_df=0.95)\n",
      "corpus = ['hi cairo here is CAIRON','here USA s 09.here in u-sa','hi in']\n",
      "corpus_vectors = vectorizer.fit_transform(corpus)\n",
      "corpus_vectors_ = sparse.csr_matrix(corpus_vectors.shape,dtype=bool)#numpy.matrix(numpy.zeros(corpus_vectors.shape))\n",
      "# apply minimum term frequency threshold\n",
      "#corpus_vectors = corpus_vectors.tolil()\n",
      "term_freq = corpus_vectors.sum(axis=0) # sum on culomns to obtain term frequencies\n",
      "terms_to_remove = []\n",
      "row = 0\n",
      "for k,v in vectorizer.vocabulary_.iteritems():\n",
      "    if(term_freq[0,vectorizer.vocabulary_[k]]>=min_tf):\n",
      "        corpus_vectors_[row,v] = 1\n",
      "    else:\n",
      "        terms_to_remove.append(k)        \n",
      "    row = row + 1\n",
      "\n",
      "corpus_vectors = corpus_vectors * corpus_vectors_\n",
      "\n",
      "for k in terms_to_remove:\n",
      "    del vectorizer.vocabulary_[k]\n",
      "\n",
      "print corpus_vectors.shape\n",
      "print vectorizer.vocabulary_\n",
      "print vectorizer.stop_words_\n",
      "#tokenizer=StermmingTokenizer(),\n",
      "#token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}